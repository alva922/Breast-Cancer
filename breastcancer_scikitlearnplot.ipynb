import os
os.chdir('YOURPATH')    # Set working directory YOURPATH
os. getcwd() 

#Import key libraries
# Import pandas
import pandas as pd

from sklearn.preprocessing import RobustScaler

import scikitplot as skplt

import sklearn
from sklearn.datasets import load_digits, load_boston, load_breast_cancer
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

import sys
import warnings
warnings.filterwarnings("ignore")

print("Scikit Plot Version : ", skplt.__version__)
print("Scikit Learn Version : ", sklearn.__version__)
print("Python Version : ", sys.version)

%matplotlib inline

#Loading BC dataset
from sklearn import datasets
data = datasets.load_breast_cancer()

print(data.keys())

print(data.DESCR) 
print(data.target_names)
print(data.feature_names)

# Read the DataFrame, first using the feature data
df = pd.DataFrame(data.data, columns=data.feature_names)
# Add a target column, and fill it with the target data
df['target'] = data.target
# Show the first five rows as 5 rows Ã— 31 columns
df.head()
df.info()

# Store the feature data
X = data.data
# store the target data
y = data.target
# split the data using Scikit-Learn's train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)

#data scaling
ss_train = RobustScaler()
X_train = ss_train.fit_transform(X_train)

ss_test = RobustScaler()
X_test = ss_test.fit_transform(X_test)

#KNN learning curve
from sklearn.neighbors import KNeighborsClassifier
logreg = KNeighborsClassifier(n_neighbors=6)
logreg.fit(X_train, y_train)
logreg.score(X_test, y_test)
skplt.estimators.plot_learning_curve(logreg, X_test, y_test,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="KNeighborsClassifier(n_neighbors=6) Test Learning Curve");
plt.savefig('learningcurveknn.png', dpi=300, bbox_inches='tight')

skplt.estimators.plot_learning_curve(logreg, X_train, y_train,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="KNeighborsClassifier(n_neighbors=6) Train Learning Curve");
plt.savefig('learningcurveknntrain.png', dpi=300, bbox_inches='tight')

#Random Foreest learning curve

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf.score(X_test, y_test)

skplt.estimators.plot_learning_curve(rf, X_train, y_train,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="RandomForestClassifier() Train Learning Curve");
plt.savefig('learningcurverfctrain.png', dpi=300, bbox_inches='tight')

skplt.estimators.plot_learning_curve(rf, X_test, y_test,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="RandomForestClassifier() Test Learning Curve");
plt.savefig('learningcurverfctest.png', dpi=300, bbox_inches='tight')

#Gradient Boosting learning curve

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
gb.score(X_test, y_test)

skplt.estimators.plot_learning_curve(gb, X_train, y_train,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="GradientBoostingClassifier() Train Learning Curve");
plt.savefig('learningcurvegbctrain.png', dpi=300, bbox_inches='tight')

skplt.estimators.plot_learning_curve(gb, X_test, y_test,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="GradientBoostingClassifier() Test Learning Curve");
plt.savefig('learningcurvegbctest.png', dpi=300, bbox_inches='tight')

#Extra Trees learning curve

xt = ExtraTreesClassifier()
xt.fit(X_train, y_train)
xt.score(X_test, y_test)

skplt.estimators.plot_learning_curve(xt, X_train, y_train,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="ExtraTreesClassifier() Train Learning Curve");
plt.savefig('learningcurveetctrain.png', dpi=300, bbox_inches='tight')

skplt.estimators.plot_learning_curve(xt, X_test, y_test,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="ExtraTreesClassifier() Test Learning Curve");
plt.savefig('learningcurveetctest.png', dpi=300, bbox_inches='tight')

#Logistic regression learning curve
lr = LogisticRegression()
lr.fit(X_train, y_train)
lr.score(X_test, y_test)

skplt.estimators.plot_learning_curve(lr, X_train, y_train,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="LogisticRegression() Train Learning Curve");
plt.savefig('learningcurvelrtrain.png', dpi=300, bbox_inches='tight')

skplt.estimators.plot_learning_curve(lr, X_test, y_test,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="LogisticRegression() Test Learning Curve");
plt.savefig('learningcurvelrtest.png', dpi=300, bbox_inches='tight')

#Dominant features

#RF vs ET
fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.estimators.plot_feature_importances(rf, feature_names=data.feature_names,
                                         title="Random Forest Classifier Feature Importance",
                                         x_tick_rotation=90, order="ascending",
                                         ax=ax1);

ax2 = fig.add_subplot(122)
skplt.estimators.plot_feature_importances(xt, feature_names=data.feature_names,
                                         title="Extra Trees Classifier Feature Importance",
                                         x_tick_rotation=90,
                                         ax=ax2);

plt.tight_layout()
plt.savefig('featureimportancerfxt.png', dpi=300, bbox_inches='tight')

#RF vs GB

fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.estimators.plot_feature_importances(rf, feature_names=data.feature_names,
                                         title="RandomForest Feature Importance",
                                         x_tick_rotation=90, order="ascending",
                                         ax=ax1);

ax2 = fig.add_subplot(122)
skplt.estimators.plot_feature_importances(gb, feature_names=data.feature_names,
                                         title="Gradient Boosting Classifier Feature Importance",
                                         x_tick_rotation=90,
                                         ax=ax2);

plt.tight_layout()
plt.savefig('featureimportancerfgb.png', dpi=300, bbox_inches='tight')

#Calibration curves

lr_probas = LogisticRegression().fit(X_train, y_train).predict_proba(X_test)
rf_probas = RandomForestClassifier().fit(X_train, y_train).predict_proba(X_test)
gb_probas = GradientBoostingClassifier().fit(X_train, y_train).predict_proba(X_test)
et_scores = ExtraTreesClassifier().fit(X_train, y_train).predict_proba(X_test)
kn_scores = KNeighborsClassifier(n_neighbors=6).fit(X_train, y_train).predict_proba(X_test)

probas_list = [lr_probas, rf_probas, gb_probas, et_scores,kn_scores]
clf_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'Extra Trees Classifier','KNeighborsClassifier']

kplt.metrics.plot_calibration_curve(y_test,
                                     probas_list,
                                     clf_names, n_bins=15,
                                     figsize=(12,6)
                                     );
plt.savefig('calibrationcurves.png', dpi=300, bbox_inches='tight')

#LR vs ET confusion matrix

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_test_pred_lr = lr.predict(X_test)
xt=ExtraTreesClassifier()
xt.fit(X_train, y_train)
y_test_pred_xt = xt.predict(X_test)
fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_lr,
                                    title="LogisticRegression Confusion Matrix",
                                    cmap="Oranges",normalize='all',
                                    ax=ax1)

ax2 = fig.add_subplot(122)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_xt,
                                    normalize='all',
                                    title="ExtraTreesClassifier Confusion Matrix",
                                    cmap="Purples",
                                    ax=ax2);
plt.savefig('confusionmatriceslrxt.png', dpi=300, bbox_inches='tight')

#LR vs GB confusion matrix

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_test_pred_lr = lr.predict(X_test)
gb=GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_test_pred_gb = gb.predict(X_test)
fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_lr,
                                    title="LogisticRegression Confusion Matrix",
                                    cmap="Oranges",normalize='all',
                                    ax=ax1)

ax2 = fig.add_subplot(122)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_gb,
                                    normalize='all',
                                    title="GradientBoostingClassifier Confusion Matrix",
                                    cmap="Purples",
                                    ax=ax2);
plt.savefig('confusionmatriceslrg

#RF vs KNN confusion matrix

logreg = KNeighborsClassifier(n_neighbors=6)
logreg.fit(X_train, y_train)
y_test_pred_knn = logreg.predict(X_test)

rf.fit(X_train, y_train)
y_test_pred_rf = rf.predict(X_test)

fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_rf,
                                    title="RandomForest Confusion Matrix",
                                    cmap="Oranges",normalize='all',
                                    ax=ax1)

ax2 = fig.add_subplot(122)
skplt.metrics.plot_confusion_matrix(y_test, y_test_pred_knn,
                                    normalize='all',
                                    title="KNeighbors Confusion Matrix",
                                    cmap="Purples",
                                    ax=ax2);
plt.savefig('confusionmatricesrfknn.png', dpi=300, bbox_inches='tight')

#LR ROC curve
y_test_probs = lr.predict_proba(X_test)

skplt.metrics.plot_roc_curve(y_test, y_test_probs,
                       title="Logistic Regression ROC Curve", figsize=(12,6));
plt.savefig('roclr.png', dpi=300, bbox_inches='tight')

#ET ROC curve

y_test_probs = xt.predict_proba(X_test)

skplt.metrics.plot_roc_curve(y_test, y_test_probs,
                       title="Extra Trees ROC Curve", figsize=(12,6));
plt.savefig('rocxt.png', dpi=300, bbox_inches='tight')

#GB ROC curve

gb=GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_test_probs = gb.predict_proba(X_test)

skplt.metrics.plot_roc_curve(y_test, y_test_probs,
                       title="Gradient Boosting ROC Curve", figsize=(12,6));
plt.savefig('rocgb.png', dpi=300, bbox_inches='tight')

#KNN ROC curve

from sklearn.neighbors import KNeighborsClassifier
logreg = KNeighborsClassifier(n_neighbors=6)
logreg.fit(X_train, y_train)
logreg.score(X_test, y_test)

y_test_probs = logreg.predict_proba(X_test)

skplt.metrics.plot_roc_curve(y_test, y_test_probs,
                       title="KNeighbors ROC Curve", figsize=(12,6));
plt.savefig('rocknn.png', dpi=300, bbox_inches='tight')

#RF ROC curve

y_test_probs = rf.predict_proba(X_test)

skplt.metrics.plot_roc_curve(y_test, y_test_probs,
                       title="RandomForest ROC Curve", figsize=(12,6));
plt.savefig('rocrfc.png', dpi=300, bbox_inches='tight')

#LR precision-recall curve

y_test_probs = lr.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_test_probs,
                       title="Logistic Regression Precision-Recall Curve", figsize=(12,6));
plt.savefig('precision-recalllr.png', dpi=300, bbox_inches='tight')

#ET precision-recall curve
y_test_probs = xt.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_test_probs,
                       title="Extra Trees Precision-Recall Curve", figsize=(12,6));
plt.savefig('precision-recallxt.png', dpi=300, bbox_inches='tight')

#Gradient Boosting Precision-Recall Curve

y_test_probs = gb.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_test_probs,
                       title="Gradient Boosting Precision-Recall Curve", figsize=(12,6));
plt.savefig('precision-recallgb.png', dpi=300, bbox_inches='tight')

#KNeighbors Precision-Recall Curve

y_test_probs = logreg.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_test_probs,
                       title="KNeighbors Precision-Recall Curve", figsize=(12,6));
plt.savefig('precision-recallknn.png', dpi=300, bbox_inches='tight')


#RandomForest Precision-Recall Curve

y_test_probs = rf.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_test_probs,
                       title="RandomForest Precision-Recall Curve", figsize=(12,6));
plt.savefig('precision-recallrfc.png', dpi=300, bbox_inches='tight')


#Logistic Regression KS Statistic Plot

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_test_pred_proba_lr = lr.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_lr, figsize=(10,6),title="Logistic Regression KS Statistic Plot");
plt.savefig('kstatlr.png', dpi=300, bbox_inches='tight')

#Extra Trees KS Statistic Plot

xt = ExtraTreesClassifier()
xt.fit(X_train, y_train)
y_test_pred_proba_xt = xt.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_xt, figsize=(10,6),title="Extra Trees KS Statistic Plot");
plt.savefig('kstatxt.png', dpi=300, bbox_inches='tight')


#Gradient Boosting KS Statistic Plot

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_test_pred_proba_gb = gb.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_gb, figsize=(10,6),title="Gradient Boosting KS Statistic Plot");
plt.savefig('kstatgb.png', dpi=300, bbox_inches='tight')


#Random Forest KS Statistic Plot

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_test_pred_proba_rf = rf.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_rf, figsize=(10,6),title="Random Forest KS Statistic Plot");
plt.savefig('kstatrf.png', dpi=300, bbox_inches='tight')


#KNeighbors KS Statistic Plot

logreg = KNeighborsClassifier(n_neighbors=6)
logreg.fit(X_train, y_train)
y_test_pred_proba_knn = logreg.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_knn, figsize=(10,6),title="KNeighbors KS Statistic Plot");
plt.savefig('kstatknn.png', dpi=300, bbox_inches='tight')

#RandomForest KS Statistic Plot

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_test_pred_proba_rf = rf.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_test_pred_proba_rf, figsize=(10,6),title="RandomForest KS Statistic Plot");
plt.savefig('kstatrfc.png', dpi=300, bbox_inches='tight')

#Logistic Regression Cumulative Gains Curve

lr_probas = LogisticRegression().fit(X_train, y_train).predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, lr_probas, figsize=(10,6),title="Logistic Regression Cumulative Gains Curve");
plt.savefig('cumgainlr.png', dpi=300, bbox_inches='tight')

#Extra Trees Cumulative Gains Curve

xt_probas = ExtraTreesClassifier().fit(X_train, y_train).predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, xt_probas, figsize=(10,6),title="Extra Trees Cumulative Gains Curve");
plt.savefig('cumgainxt.png', dpi=300, bbox_inches='tight')

#Gradient Boosting Cumulative Gains Curve

gb_probas = GradientBoostingClassifier().fit(X_train, y_train).predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, gb_probas, figsize=(10,6),title="Gradient Boosting Cumulative Gains Curve");
plt.savefig('cumgaingb.png', dpi=300, bbox_inches='tight')

#Random Forest Cumulative Gains Curve

rf_probas = RandomForestClassifier().fit(X_train, y_train).predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, rf_probas, figsize=(10,6),title="Random Forest Cumulative Gains Curve");
plt.savefig('cumgainrf.png', dpi=300, bbox_inches='tight')

#KNeighbors Cumulative Gains Curve

skplt.metrics.plot_cumulative_gain(y_test, kn_scores, figsize=(10,6),title="KNeighbors Cumulative Gains Curve");
plt.savefig('cumgainknn.png', dpi=300, bbox_inches='tight')

#RandomForest Cumulative Gains Curve

skplt.metrics.plot_cumulative_gain(y_test, rf_probas, figsize=(10,6),title="RandomForest Cumulative Gains Curve");
plt.savefig('cumgainrfc.png', dpi=300, bbox_inches='tight')

#Logistic Regression Lift Curve

skplt.metrics.plot_lift_curve(y_test, lr_probas, figsize=(10,6),title=" Logistic Regression Lift Curve");
plt.savefig('liftlr.png', dpi=300, bbox_inches='tight')

#Extra Trees Lift Curve

xt = ExtraTreesClassifier()
xt.fit(X_train, y_train)
y_test_pred_proba_xt = xt.predict_proba(X_test)
skplt.metrics.plot_lift_curve(y_test, y_test_pred_proba_xt, figsize=(10,6),title=" Extra Trees Lift Curve");
plt.savefig('liftxt.png', dpi=300, bbox_inches='tight')

#Gradient Boosting Lift Curve
gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_test_pred_proba_gb = gb.predict_proba(X_test)
skplt.metrics.plot_lift_curve(y_test, y_test_pred_proba_gb, figsize=(10,6),title=" Gradient Boosting Lift Curve");
plt.savefig('liftgb.png', dpi=300, bbox_inches='tight')

#Random Forest Lift Curve

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_test_pred_proba_rf = rf.predict_proba(X_test)
skplt.metrics.plot_lift_curve(y_test, y_test_pred_proba_rf, figsize=(10,6),title=" Random Forest Lift Curve");
plt.savefig('liftrf.png', dpi=300, bbox_inches='tight')

#KNeighbors Lift Curve

skplt.metrics.plot_lift_curve(y_test, kn_scores, figsize=(10,6),title=" KNeighbors Lift Curve");
plt.savefig('liftknn.png', dpi=300, bbox_inches='tight')

#Elbow Plot

skplt.cluster.plot_elbow_curve(KMeans(random_state=1),
                               X,
                               cluster_ranges=range(2, 20),
                               figsize=(8,6));
plt.savefig('elbowplot.png', dpi=300, bbox_inches='tight')

#Silhouette Analysis

kmeans = KMeans(n_clusters=10, random_state=1)
kmeans.fit(X_train, y_train)
cluster_labels = kmeans.predict(X_test)
skplt.metrics.plot_silhouette(X_test, cluster_labels,
                              figsize=(8,6));
plt.savefig('silhouette.png', dpi=300, bbox_inches='tight')

#PCA

pca = PCA(random_state=1)
pca.fit(X)

skplt.decomposition.plot_pca_component_variance(pca, figsize=(8,6));
plt.savefig('pcavariances.png', dpi=300, bbox_inches='tight')

#PCA 2-D Projection
skplt.decomposition.plot_pca_2d_projection(pca, X, y,
                                           figsize=(10,10),
                                           cmap="tab10");
plt.savefig('pca2dprojection.png', dpi=300, bbox_inches='tight')

# COntinue by importing all necessary libraries
import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

#LR classification report

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_test_pred_lr = lr.predict(X_test)

print(accuracy_score(y_test_pred_lr, y_test))
print(confusion_matrix(y_test_pred_lr, y_test))
print(classification_report(y_test_pred_lr, y_test))

from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score


class_names = ['0', '1']


# Run classifier, using a model that is too regularized (C too low) to see
# the impact on the results
classifier = lr.fit(X_train, y_train)

np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
titles_options = [
    ("LR Confusion matrix, without normalization", None),
    ("LR Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

#plt.show()
plt.savefig('confmatrixlr.png', dpi=300, bbox_inches='tight')

#Extra Trees classification report

xt = ExtraTreesClassifier()
xt.fit(X_train, y_train)
y_test_pred_xt = xt.predict(X_test)

print(accuracy_score(y_test_pred_xt, y_test))
print(confusion_matrix(y_test_pred_xt, y_test))
print(classification_report(y_test_pred_xt, y_test))

classifier = xt.fit(X_train, y_train)
titles_options = [
    ("ExtraTrees Confusion matrix, without normalization", None),
    ("ExtraTrees Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.savefig('confmatrixxt.png', dpi=300, bbox_inches='tight')

#GB classification report

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_test_pred_gb = gb.predict(X_test)

print(accuracy_score(y_test_pred_gb, y_test))
print(confusion_matrix(y_test_pred_gb, y_test))
print(classification_report(y_test_pred_gb, y_test))

classifier = gb.fit(X_train, y_train)
titles_options = [
    ("Gradient Boosting Confusion Matrix, without normalization", None),
    ("Gradient Boosting Confusion Matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.savefig('confmatrixgb.png', dpi=300, bbox_inches='tight')

#RF classification report

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_test_pred_rf = rf.predict(X_test)

print(accuracy_score(y_test_pred_rf, y_test))
print(confusion_matrix(y_test_pred_rf, y_test))
print(classification_report(y_test_pred_rf, y_test))

classifier = rf.fit(X_train, y_train)
titles_options = [
    ("Random Forest Confusion Matrix, without normalization", None),
    ("Random Forest Confusion Matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.savefig('confmatrixrf.png', dpi=300, bbox_inches='tight')

#KNN classification report

knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(X_train, y_train)
y_test_pred_knn = knn.predict(X_test)

print(accuracy_score(y_test_pred_knn, y_test))
print(confusion_matrix(y_test_pred_knn, y_test))
print(classification_report(y_test_pred_knn, y_test))

classifier = knn.fit(X_train, y_train)
titles_options = [
    ("KNN Confusion Matrix, without normalization", None),
    ("KNN Confusion Matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.savefig('confmatrixknn.png', dpi=300, bbox_inches='tight')








